# Tic-Tac-Toe PPO Agent

A Transformer-based reinforcement learning agent for Tic-Tac-Toe, trained using Proximal Policy Optimization (PPO). This project includes training utilities, REST API endpoints, and a simple web interface for playing against the agent. An example gameplay session with the trained agent is shown below.

<p align="center">
    <br>
        <img alt="img-name" src="assets/game.gif" width="450">
    <br>
        <em>Playing against the trained agent.</em>
    <br>
</p>


### ğŸš€ Features
- PPO-based agent with Generalized Advantage Estimation (GAE)
- Multiple opponent types: random, rule-based, and self-play
- REST API powered by FastAPI for training and gameplay
- Save and load the best-performing agent checkpoint
- Web interface for human-agent interaction
- Train the agent by interacting through the web interface
- Live training metric logging
- Full unit test suite


### ğŸ§© Project Structure
```bash
.
â”œâ”€â”€ src/tictactoe/           # Core logic (agents, model, environment, API)
â”‚   â”œâ”€â”€ agents/              # Agent implementations
â”‚   â”œâ”€â”€ api/                 # FastAPI app and API routes
â”‚   â”œâ”€â”€ model.py             # Actor-Critic model
â”‚   â”œâ”€â”€ environment.py       # Game environment and rewards
â”‚   â”œâ”€â”€ trainer.py           # Training loop logic
â”‚   â””â”€â”€ config.py            # Configuration parameters
â”œâ”€â”€ frontend/                # Web interface (HTML + JS)
â”œâ”€â”€ tests/                   # Unit tests for key components
â”œâ”€â”€ checkpoints/             # Saved agent models
â”œâ”€â”€ .devcontainer/           # VS Code Dev Container setup
â”œâ”€â”€ Dockerfile               # Docker container
â”œâ”€â”€ docker-compose.yml       # Full-stack orchestration
â”œâ”€â”€ pyproject.toml           # Poetry configuration
```

### â–¶ï¸ Quick Start

Ensure you have Docker and Docker Compose installed. Then launch the app:
```bash
docker-compose up --build
```

The game UI will be available at: `http://localhost:8000`.


### ğŸ® UI Overview
The web UI allows you to interact with the agent and control training directly
from your browser (see screenshot below).

<p align="center">
    <br>
        <img alt="img-name" src="assets/web_interface.png" width="450">
    <br>
        <em>Tic-Tac-Toe user interface.</em>
    <br>
</p>

UI Components:

1. **Game Board**
    - 3Ã—3 interactive grid for playing against the agent.
    - You always play first.

2. `â†» Next round` **button**
    - Resets the board and starts a new round after a game ends.
    - In the next round, the opponent goes first.

3. **Checkpoint Controls**
    - `â†“ Save Checkpoint`: Saves the current best agent to disk.
    - `â†‘ Load Checkpoint`: Loads the best saved agent from disk.

4. **Training Controls**
    - `â–¶ Run Training`: Starts PPO agent training.
    - `âš Random Agent`: Opponent playing randomly.
    - `âš™ Rule-Based Agent`: Opponent using simple heuristics.
    - `â†º Self-Play`: Agent plays against a copy of itself.

5. **Training Progress Graph**
    - Displays the win rate of the PPO agent across training steps (max 100 games per point).
    - Red line shows the highest recorded win rate.
    - The graph updates in a sliding-window manner, displaying only the latest 1000 points.


### âš™ï¸ Model Architecture

The agent is based on a dual-encoder Transformer Actor-Critic architecture, implemented in PyTorch.
The actor and critic use separate encoders and heads, allowing them to learn specialized
representations of the board state.

```
        â”Œâ”€â”€â–º Embeddings â”€â–º Transformer â”€â–º Pooling + GELU(FC) â”€â–º Actor Head â”€â–º Action Logits
Input â”€â”€â”¤
        â””â”€â”€â–º Embeddings â”€â–º Transformer â”€â–º Pooling + GELU(FC) â”€â–º Critic Head â”€â–º Value Estimate
```

- **Input**:
    - The game board is encoded as a flat sequence of 9 integers (3Ã—3), where each cell represents one of three states:
    `0 = empty`, `1 = opponent`, `2 = agent`.

- **Actor Encoder**:
    - Embedding layer for cell states.
    - Sinusoidal positional encoding.
    - Transformer Encoder stack (configurable depth, heads, width).
    - Sum-polling and linear projection with activation.

- **Critic Encoder**:
    - Mirrors the actor's architecture but with independent parameters. This allows
    the critic to learn a distinct representation of the board for value estimation.

- **Heads**:
    - Actor Head: Produces logits over all board positions. Invalid moves
    (non-empty cells) are masked during action selection.
    - Critic Head: Outputs a scalar value estimating the state's value (expected future reward).

- **Training**:
    - Proximal Policy Optimization.
    - Optional: Generalized Advantage Estimation.
    - Optional: reward normalization, advantage normalization, advantage centralization.
    - Optional: policy entropy regularization.
    - Actor and critic are trained independently with separate optimizers, using shared trajectory memory.

This architecture enables independent learning of policy and value functions 
while leveraging Transformer expressiveness.

### âœ… Running Tests
```bash
poetry run pytest
```

### ğŸ“„ License
[MIT License](https://github.com/artem-gorodetskii/Tic-Tac-Toe-PPO/blob/main/LICENSE)
